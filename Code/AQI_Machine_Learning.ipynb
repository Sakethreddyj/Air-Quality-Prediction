{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7515d8d-0945-4050-b5c2-be10be99cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a2e011-86fa-4e45-8c7a-8ecc328e2ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a990d7-0108-4635-88ae-c48bdfeb3a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b85f805-4eba-4cbe-aa04-99204f37917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the air quality data \n",
    "air_quality_data = pd.read_csv(r'C:\\Users\\tvams\\OneDrive\\Desktop\\Imputation methods\\Step-3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8dd08b-823f-4dda-9583-6c15414d2799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform min-max normalization\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(air_quality_data.drop(columns=['Date','AQI']))\n",
    "X_normalized = normalized_data\n",
    "y = air_quality_data['AQI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab4445a-1972-459c-aa50-122515a5531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c06e5ae-e5b7-4049-a7e0-377189c0c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'SVM ': SVR(),\n",
    "    'KNN ': KNeighborsRegressor()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3810a5-9fc4-41a9-b234-99bc94686664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "metrics = {\n",
    "    'MAE': mean_absolute_error,\n",
    "    'MSE': mean_squared_error,\n",
    "    'RMSE': lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "    'R2 Score': r2_score\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99863dcd-72ef-41f0-ae98-85a6dd624740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform k-fold cross-validation and evaluate each model\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f'Running {model_name}...')\n",
    "    start_time = time.time()\n",
    "    scores = {'Time': [], 'MAE': [], 'MSE': [], 'RMSE': [], 'R2 Score': []}\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        X_train_fold, X_val_fold = X_train[train_index], X_train[test_index]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        y_pred = model.predict(X_val_fold)\n",
    "        end_time = time.time()\n",
    "        time_taken = end_time - start_time\n",
    "        scores['Time'].append(time_taken)\n",
    "        for metric_name, metric_func in metrics.items():\n",
    "            if metric_name != 'Time':\n",
    "                scores[metric_name].append(metric_func(y_val_fold, y_pred))\n",
    "    results[model_name] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d404e9d2-14ea-4c4d-99f9-bb7927bf7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print evaluation results\n",
    "for model_name, scores in results.items():\n",
    "    print(f'\\n{model_name}:')\n",
    "    for metric_name, metric_values in scores.items():\n",
    "        if metric_name == 'Time':\n",
    "            print(f'Time: {np.mean(metric_values):.4f} seconds')\n",
    "        else:\n",
    "            print(f'{metric_name}: {np.mean(metric_values):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9823fd9b-a4c2-4236-8706-194ce64d46e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot actual versus predicted results for each model\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, (model_name, model) in enumerate(models.items(), 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    plt.scatter(y_test, y_pred)\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\n",
    "    plt.xlabel('Actual AQI')\n",
    "    plt.ylabel('Predicted AQI')\n",
    "    plt.title(f'Actual vs Predicted AQI ({model_name})')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0847943-b86a-46da-a426-f9ac2461388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual versus predicted results for each model\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, (model_name, model) in enumerate(models.items(), 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    plt.plot(y_test.values, label='Actual', color='blue')\n",
    "    plt.plot(y_pred, label='Predicted', color='red')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('AQI')\n",
    "    plt.title(f'Actual vs Predicted AQI ({model_name})')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f706dff0-e3a5-432c-973a-f3537f1b385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual AQI values against dates\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(air_quality_data['Date'], air_quality_data['AQI'], color='blue')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('AQI')\n",
    "plt.title('Actual AQI over Time')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd7c1ee-d0e5-456f-af7a-c2f2ac0d67b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date column to datetime format with correct format\n",
    "air_quality_data['Date'] = pd.to_datetime(air_quality_data['Date'], format='%Y-%m-%d')\n",
    "\n",
    "# Extract year from Date column\n",
    "air_quality_data['Year'] = air_quality_data['Date'].dt.year\n",
    "\n",
    "# Calculate average AQI for each year\n",
    "average_aqi_by_year = air_quality_data.groupby('Year')['AQI'].mean()\n",
    "\n",
    "# Plot average AQI values against years\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(average_aqi_by_year.index, average_aqi_by_year.values, color='blue', marker='o', linestyle='-')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average AQI')\n",
    "plt.title('Average AQI by Year')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed88daa-41d1-443a-ae0b-756198a838d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the Excel file and extract data\n",
    "comparision = pd.read_excel(r'C:\\Users\\tvams\\OneDrive\\Desktop\\Imputation methods\\Comparison1.xlsx')\n",
    "\n",
    "methods = comparision['Models'].tolist()\n",
    "rmse_values = comparision['RMSE'].tolist()\n",
    "mse_values = comparision['MSE'].tolist()\n",
    "mae_values = comparision['MAE'].tolist()\n",
    "r_squared_values = comparision['R2 Score'].tolist()\n",
    "\n",
    "# Width of each bar\n",
    "bar_width = 0.15\n",
    "index = np.arange(len(methods))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "plt.bar(index - 1.5 * bar_width, rmse_values, bar_width, label='RMSE', color='blue')\n",
    "plt.bar(index - 0.5 * bar_width, mse_values, bar_width, label='MSE', color='green')\n",
    "plt.bar(index + 0.5 * bar_width, mae_values, bar_width, label='MAE', color='orange')\n",
    "plt.bar(index + 1.5 * bar_width, r_squared_values, bar_width, label='R-squared', color='red')\n",
    "\n",
    "plt.xlabel('Methods')\n",
    "plt.ylabel('Error Value')\n",
    "plt.title('Comparison of Methods by Evaluation  Metrics')\n",
    "plt.xticks(index, methods)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Separate plots for each metric\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# RMSE Plot\n",
    "plt.bar(methods, rmse_values, color='blue')\n",
    "plt.xlabel('Methods')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Comparison of Methods by RMSE')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# MSE Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(methods, mse_values, color='green')\n",
    "plt.xlabel('Methods')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Comparison of Methods by MSE')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# MAE Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(methods, mae_values, color='orange')\n",
    "plt.xlabel('Methods')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('Comparison of Methods by MAE')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# R-squared Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(methods, r_squared_values, color='red')\n",
    "plt.xlabel('Methods')\n",
    "plt.ylabel('R-squared')\n",
    "plt.title('Comparison of Methods by R-squared')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70255507-0940-4fb7-8d0c-eac00ade4b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
